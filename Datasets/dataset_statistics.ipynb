{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DATASETS = ['quickdraw', 'fashion']\n",
    "\n",
    "def image_dataset_to_array(dataset_path):\n",
    "    # Convert image to np array\n",
    "    # Preload images to memory (trying to speed things up)\n",
    "    all_files = glob.glob('{}*'.format(dataset_path))\n",
    "    # Gather ids and timestep info    \n",
    "    max_t = {}\n",
    "    for f in all_files:\n",
    "        regex = r\".*/{}/(.*-.*)-(.*).png\".format(dataset_id)\n",
    "        match = re.match(regex, f)\n",
    "        img_id, t = match.groups()\n",
    "        t = int(t)\n",
    "        max_t[img_id] = max_t[img_id] if img_id in max_t and max_t[img_id] > t else t   \n",
    "    \n",
    "    img_size = 28 * 28  # Pixel count\n",
    "    n_revisions = max(max_t.values()) + 1\n",
    "    n_items = len(max_t.values())\n",
    "    vs = np.empty((n_revisions, n_items, img_size))\n",
    "    \n",
    "    # Populate vs\n",
    "    for i, img_id in enumerate(natsorted(max_t)):\n",
    "        # Copy existing bitmaps to np.array\n",
    "        for t in range(0, max_t[img_id]):\n",
    "            img_file = dataset_path + img_id + '-' + str(t) + '.png'\n",
    "            vs[t][i] = (cv2.imread(img_file, cv2.IMREAD_GRAYSCALE) / 255.).flatten()\n",
    "        # Replicate last image\n",
    "        for t in range(max_t[img_id], n_revisions):\n",
    "            img_file = dataset_path + img_id + '-' + str(max_t[img_id]-1) + '.png'\n",
    "            vs[t][i] = (cv2.imread(img_file, cv2.IMREAD_GRAYSCALE) / 255.).flatten()    \n",
    "    return vs, list(natsorted(max_t)), n_revisions\n",
    "\n",
    "\n",
    "def tabular_dataset_to_array(dataset_path):\n",
    "    # Get files with coords and save in an array vs\n",
    "    all_files = natsorted(glob.glob('{}*'.format(dataset_path)))\n",
    "    vs = [pd.read_csv(f, index_col=0).values for f in all_files] \n",
    "    # Get dataset info \n",
    "    df_temp = pd.read_csv(all_files[0], index_col=0)\n",
    "    n_timesteps = len(all_files)\n",
    "    return np.array(vs), list(df_temp.index), n_timesteps\n",
    "\n",
    "\n",
    "def dataset_as_array(dataset_path):\n",
    "    if dataset_id in IMAGE_DATASETS:\n",
    "        return image_dataset_to_array(dataset_path)\n",
    "    else:\n",
    "        return tabular_dataset_to_array(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_id, avg_intrinsic_dim, avg_sparsity\n",
      "cartolastd 0.6470588235294117 0.0\n",
      "cifar10cnn 0.6599999999999997 0.0\n",
      "esc50 0.03457754629629635 0.0\n",
      "fashion 0.47627551020408165 0.29714502551020405\n",
      "gaussians 0.36800000000000005 0.0\n",
      "nnset 0.005753820735233375 0.00012391573729863693\n",
      "qtables 0.007750000000000007 0.0007428240740740739\n",
      "quickdraw 0.4309504700756699 0.9013996455323707\n",
      "sorts 0.35051020408163236 0.010068877551020409\n",
      "walk 0.47839999999999994 0.00019999999999999998\n"
     ]
    }
   ],
   "source": [
    "datasets = !cat datasets.txt\n",
    "\n",
    "print('dataset_id, avg_intrinsic_dim, avg_sparsity')\n",
    "for dataset_id in datasets:\n",
    "    vs, indexes, _ = dataset_as_array('./' + dataset_id + '/')\n",
    "    n_timesteps, n_observations, n_dimensions = vs.shape\n",
    "    avg_intrinsic_dim = 0  # averaged over all timesteps\n",
    "    avg_sparsity = 0  # averaged over all timesteps\n",
    "\n",
    "    for X in vs:\n",
    "        pca = PCA()\n",
    "        pca.fit(X) \n",
    "        cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "        avg_intrinsic_dim += (sum(cumsum < 0.95) / n_dimensions) / n_timesteps\n",
    "        avg_sparsity += np.sum(X == 0) / (X.shape[0] * X.shape[1]) / n_timesteps\n",
    "\n",
    "    print(dataset_id, avg_intrinsic_dim, avg_sparsity)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
