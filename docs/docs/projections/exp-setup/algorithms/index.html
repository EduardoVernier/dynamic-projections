<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content><meta property="og:description" content="Algorithms PCA - A technique for dimensionality reduction that performs a linear mapping of the data to a lower-dimensional space maximizing the variance of the data in the low-dimensional representation. We created a wrapper that around the scikit-learn implementation that offers two usage modes: Strategy TF (pca_s1) computes PCA independently for each timestep. Strategy G (pca_s4) works by grouping all timesteps and computing PCA once. The terminology was borrowed from the dt-SNE paper."><meta property="og:type" content="article"><meta property="og:url" content="https://eduardovernier.github.io/dynamic-projections/docs/projections/exp-setup/algorithms/"><title>Algorithms | Dynamic projections</title><link rel=icon href=/dynamic-projections/favicon.png type=image/x-icon><link rel=stylesheet href=/dynamic-projections/book.min.e095d2fc282039b2e0a50cefad944330a73f934c3bd2b9272bb3fdfeb8f4f34d.css integrity="sha256-4JXS/CggObLgpQzvrZRDMKc/k0w70rknK7P9/rj0800="></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><h2 class=book-brand><a href=https://eduardovernier.github.io/dynamic-projections/><span>Dynamic projections</span></a></h2><ul><li class=book-section-flat><span>Experimental setup</span><ul><li><a href=/dynamic-projections/docs/projections/exp-setup/algorithms/ class=active>Algorithms</a></li><li><a href=/dynamic-projections/docs/projections/exp-setup/datasets/>Datasets</a></li><li><a href=/dynamic-projections/docs/projections/exp-setup/metrics/>Metrics</a></li></ul></li><li class=book-section-flat><span>Results</span><ul><li><a href=/dynamic-projections/docs/projections/results/raw-output/>Raw output</a></li><li><a href=/dynamic-projections/docs/projections/results/videos-and-trails/>Videos and Trails</a></li></ul></li><li><a href=/dynamic-projections/docs/projections/replication/>Replication</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class="flex align-center justify-between book-header"><label for=menu-control><img src=/dynamic-projections/svg/menu.svg alt=Menu></label>
<strong>Algorithms</strong></header><article class=markdown><h1 id=algorithms>Algorithms</h1><p><strong>PCA</strong> - A technique for dimensionality reduction that performs a linear mapping of the data to a lower-dimensional space maximizing the variance of the data in the low-dimensional representation. We created a wrapper that around the scikit-learn implementation that offers two usage modes: Strategy TF (pca_s1) computes PCA independently for each timestep. Strategy G (pca_s4) works by grouping all timesteps and computing PCA once. The terminology was borrowed from the dt-SNE paper.</p><p><strong>t-SNE</strong> - This method converts the nD distances between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional mD embedding and the high-dimensional nD data. This usually results in good neighborhood preservation. Our implementation is based off the scikit-learn implementation and the perplexity is set as default (30).</p><p><strong>UMAP</strong> - This recent DR technique has a mathematical foundation on Riemannian geometry and algebraic topology. According to recent studies [EMK ∗ 19, BMH ∗ 19], UMAP offers high quality projections with lower computational cost and better global
structure preservation than t-SNE, being thus an interesting competitor in the DR arena. We consider in our evaluation both the global (G-UMAP) and per-timeframe (TF-UMAP) variants of this technique.</p><p><strong>dt-SNE</strong> - This method extends t-SNE to deal with dynamic data by adding a stability term (lambda) to the cost function.</p><p><strong>Autoencoders</strong> - In the context of dimensionality reduction, we take a (usually) hourglass-shaped neural network and train it to reconstruct the input. After training, the middle layer acts as a compact (latent) representation of the original data. The middle layer has to have a number of neurons equivalent to the dimensionality of the space we want to project out data into. We tested four different &ldquo;types&rdquo; of autoencoders:</p><table><thead><tr><th align=right></th><th align=left></th><th align=left></th></tr></thead><tbody><tr><td align=right>AE</td><td align=left><em>Dense autoencoders</em></td><td align=left>Fully connected layers.</td></tr><tr><td align=right>C2AE</td><td align=left><em>Convolutional autoencoders</em></td><td align=left>Used only on the image-based datasets.</td></tr><tr><td align=right>VAE</td><td align=left><em>Variational autoencoders with fully connected layers</em></td><td align=left>trying to get better internal representations by avoiding overfitting.</td></tr><tr><td align=right>C2VAE</td><td align=left><em>Variational autoencoders with convolutional layers</em></td><td align=left>possibly better of both worlds regarding input reconstruction ability.</td></tr></tbody></table><p>We experiment with different optimizers, architectures, training routines, etc. To decode the names to understand the number of layers, neurons per layer, epochs of training, etc.</p><p>The notebooks/script associated with each run can be found at <a href=https://github.com/EduardoVernier/dynamic-projections/tree/master/Models>https://github.com/EduardoVernier/dynamic-projections/tree/master/Models</a>. Information about replication and tests with new datasets can be found in the <a href=https://eduardovernier.github.io/dynamic-projections/docs/projections/replication/>Replication</a> section.</p></article></div><aside class="book-toc levels-6 fixed"><nav id=TableOfContents><ul><li><a href=#algorithms>Algorithms</a></li></ul></nav></aside></main></body></html>