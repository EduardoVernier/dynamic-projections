<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content><meta property="og:description" content="Algorithms PCA - A technique for dimensionality reduction that performs a linear mapping of the data to a lower-dimensional space maximizing the variance of the data in the low-dimensional representation. We created a wrapper that around the scikit-learn implementation that offers two usage modes: Strategy 1 (pca_s1) computes PCA independently for each timestep. Strategy 4 (pca_s4) works by grouping all timesteps and computing PCA once. The terminology was borrowed from the dt-SNE paper."><meta property="og:type" content="article"><meta property="og:url" content="https://eduardovernier.github.io/dynamic-projections/docs/projections/exp-setup/algorithms/"><title>Algorithms | Dynamic projections</title><link rel=icon href=/dynamic-projections/favicon.png type=image/x-icon><link rel=stylesheet href=/dynamic-projections/book.min.e095d2fc282039b2e0a50cefad944330a73f934c3bd2b9272bb3fdfeb8f4f34d.css integrity="sha256-4JXS/CggObLgpQzvrZRDMKc/k0w70rknK7P9/rj0800="></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><h2 class=book-brand><a href=https://eduardovernier.github.io/dynamic-projections/><span>Dynamic projections</span></a></h2><ul><li class=book-section-flat><a href=/dynamic-projections/docs/projections/exp-setup/>Experimental setup</a><ul><li><a href=/dynamic-projections/docs/projections/exp-setup/algorithms/ class=active>Algorithms</a></li><li><a href=/dynamic-projections/docs/projections/exp-setup/datasets/>Datasets</a></li><li><a href=/dynamic-projections/docs/projections/exp-setup/metrics/>Metrics</a></li></ul></li><li class=book-section-flat><a href=/dynamic-projections/docs/projections/results/>Results</a><ul><li><a href=/dynamic-projections/docs/projections/results/raw-output/>Raw output</a></li><li><a href=/dynamic-projections/docs/projections/results/videos-and-trails/>Videos and Trails</a></li></ul></li><li><a href=/dynamic-projections/docs/projections/replication/>Replication</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class="flex align-center justify-between book-header"><label for=menu-control><img src=/dynamic-projections/svg/menu.svg alt=Menu></label>
<strong>Algorithms</strong></header><article class=markdown><h1 id=algorithms>Algorithms</h1><p><strong>PCA</strong> - A technique for dimensionality reduction that performs a linear mapping of the data to a lower-dimensional space maximizing the variance of the data in the low-dimensional representation. We created a wrapper that around the scikit-learn implementation that offers two usage modes: Strategy 1 (pca_s1) computes PCA independently for each timestep. Strategy 4 (pca_s4) works by grouping all timesteps and computing PCA once. The terminology was borrowed from the dt-SNE paper.</p><p><strong>t-SNE</strong> - This method converts the nD distances between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional mD embedding and the high-dimensional nD data. This usually results in good neighborhood preservation. Our implementation is based off the scikit-learn implementation and the perplexity is set as default (30).</p><p><strong>dt-SNE</strong> - This method extends t-SNE to deal with dynamic data by adding a stability term (lambda) to the cost function.</p><p><strong>Autoencoders</strong> - In the context of dimensionality reduction, we take a (usually) hourglass-shaped neural network and train it to reconstruct the input. After training, the middle layer acts as a compact (latent) representation of the original data. The middle layer has to have a number of neurons equivalent to the dimensionality of the space we want to project out data into. We tested four different &ldquo;types&rdquo; of autoencoders:</p><table><thead><tr><th align=right></th><th align=left></th><th align=left></th></tr></thead><tbody><tr><td align=right>AE</td><td align=left><em>Dense autoencoders</em></td><td align=left>Fully connected layers.</td></tr><tr><td align=right>C2AE</td><td align=left><em>Convolutional autoencoders</em></td><td align=left>Used only on the image-based datasets.</td></tr><tr><td align=right>VAE</td><td align=left><em>Variational autoencoders with fully connected layers</em></td><td align=left>trying to get better internal representations by avoiding overfitting.</td></tr><tr><td align=right>C2VAE</td><td align=left><em>Variational autoencoders with convolutional layers</em></td><td align=left>possibly better of both worlds regarding input reconstruction ability.</td></tr></tbody></table><p>We experiment with different optimizers, architectures, training routines, etc. To decode the names to understand the number of layers, neurons per layer, epochs of training, etc.</p><h2 id=recreating-the-results-testing-new-methods-and-datasets>Recreating the results / Testing new methods and datasets</h2><p>First clone the official repository: <a href=https://github.com/EduardoVernier/dynamic-projections>https://github.com/EduardoVernier/dynamic-projections</a></p><p>Set up virtual env and dependencies using pipenv. <a href=https://pipenv.readthedocs.io/en/latest/>https://pipenv.readthedocs.io/en/latest/</a></p><pre><code>pip install pipenv
pipenv run pip install pip==18.0
pipenv install
sudo apt-get install python3-tk
</code></pre><p>To run a script use <code>pipenv run python &lt;script_name&gt;.py</code>. To open notebooks use <code>pipenv run jupyter notebook</code> or create a new shell with <code>pipenv shell</code> and then call <code>jupyter notebook</code>.</p><h2 id=generating-the-projections>Generating the projections</h2><p><strong>Autoencoders</strong> &mdash; <code>./Models/ae</code>
The notebooks contain the model implementation and training procedures used to for each run. The <code>Shared.py</code> file contains methods that might be useful for all notebooks and projection techniques e.g., saving projection, loading data.</p><p><strong>Dynamic/static t-sne</strong> &mdash; <code>./Models/tsne</code>
From the root folder, we need to add the <code>tsne</code> folder to the PYTHONPATH and then run the dtsne_wrapper script.</p><pre><code>export PYTHONPATH=${PYTHONPATH}:${PWD}/Models/tsne
python Models/tsne/dtsne_wrapper.py ./Datasets/gaussians 70 0.1
</code></pre><p>The default options are <code>n_epochs=200, sigma_iters=50</code>.</p><p>For static t-sne with strategies 1 and 4 (of the dt-sne paper):</p><pre><code>export PYTHONPATH=${PYTHONPATH}:${PWD}/Models/tsne
python Models/tsne/tsne_s1.py ./Datasets/gaussians  # or
python Models/tsne/tsne_s4.py ./Datasets/gaussians
</code></pre><p><strong>Principal component analysis</strong> &mdash; <code>./Models/pca</code></p><pre><code>export PYTHONPATH=${PYTHONPATH}:${PWD}/Models/
python Models/pca/pca_s1.py ./Datasets/gaussians  # or
python Models/pca/pca_s4.py ./Datasets/gaussians
</code></pre><p><strong>Formatting</strong></p><p><strong>Image datasets</strong> &ndash; The directory hierarchy doesnâ€™t matter, all the metadata should be contained in the file name. <code>&lt;class&gt;-&lt;id&gt;-&lt;time&gt;.png</code>, e.g. <code>airplane-1234-10.png</code> &ndash; 10th revision of airplane with id 1234.</p><p><strong>Tabular datasets</strong> &ndash; Each timestep is a single csv file named <code>&lt;dataset_name&gt;-&lt;time&gt;.csv</code>. The first column is the id and the next are the n features. I think this dtsne implementation only handles numerical features, so nothing categorical here for now.</p><p><strong>Output data (actual projections)</strong> &mdash; <code>./Output</code> -
Single csv file with information about the model in the name in the format <code>&lt;dataset&gt;-&lt;model_info&gt;.csv</code>, as in <code>quickdraw-AE_728c_200c_p_d_200f_500f_2f.csv</code>. The previous string is an hypothetical filename for the results of projection using an AE with two convolutional layers of 728 and 200 kernels each, followed by max pooling and dropout layers and three dense layers of 200, 500 and 2 neurons each. As for the contents of the file, the first column is the <code>id</code>, and the next are <code>t0d0, t0d1, ... t0dX, t1d0, ..., tTdX.</code> The number &rsquo;t&rsquo; is the timestep and &rsquo;d&rsquo; is the representation dimension of each value.</p><h3 id=visualizing-the-projections>Visualizing the projections</h3><p>There is a simple python tool based on matplotlib to quickly show and help us debug the generated projections. To use it, call</p><pre><code>python Vis/Main.py ./Output/gaussians-pca_s4.csv ./Output/gaussians-AE_10f_2f_20ep.csv
python Vis/Main.py $(find Output/ -type f -name cartolastd*)
</code></pre></article></div><aside class="book-toc levels-6 fixed"><nav id=TableOfContents><ul><li><a href=#algorithms>Algorithms</a><ul><li><a href=#recreating-the-results-testing-new-methods-and-datasets>Recreating the results / Testing new methods and datasets</a></li><li><a href=#generating-the-projections>Generating the projections</a><ul><li><a href=#visualizing-the-projections>Visualizing the projections</a></li></ul></li></ul></li></ul></nav></aside></main></body></html>