{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the parameters cell\n",
    "# projection_paths = './Output/quickdraw-pca_s4.csv'\n",
    "# projection_paths = './Output/gaussians-pca_s4.csv ./Output/gaussians-AE_10f_2f_20ep.csv'\n",
    "projection_paths = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_paths = projection_paths.split(' ')\n",
    "dataset_id = projection_paths[0].split('/')[-1].split('-')[0]\n",
    "print(projection_paths, dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import math\n",
    "import cv2\n",
    "import re\n",
    "import glob\n",
    "import sys\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# os.chdir('..')\n",
    "sys.stderr = open('Metrics/log_' + dataset_id, 'w') # to check tqdm progress followed by watch -n 1 cat logfile\n",
    "\n",
    "IMAGE_DATASETS = ['quickdraw', 'fashion']\n",
    "# K_VALUES = [.05, .1, .15, .2] \n",
    "K_VALUES = [i/10 for i in range(1,51)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_as_array(dataset_path):\n",
    "    df = pd.read_csv(dataset_path, index_col=0)\n",
    "    vs = df.values.reshape(len(df), -1, 2)\n",
    "    return vs, list(df.index), vs.shape[1]\n",
    "\n",
    "\n",
    "def get_md_mov(dataset_path):\n",
    "    vs, indexes, n_timesteps = get_projection_as_array(dataset_path)\n",
    "    mov = []\n",
    "    for poly in vs:\n",
    "        mov_i = []\n",
    "        for i in range(len(poly)-1):\n",
    "            mov_i.append(math.sqrt(np.sum(np.square(poly[i] - poly[i+1]))))\n",
    "        mov.append(np.array(mov_i))\n",
    "    return np.array(mov), indexes, n_timesteps\n",
    "\n",
    "\n",
    "# get_md_mov('./Output/quickdraw-pca_s4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_dataset_to_array(dataset_path):\n",
    "    # Convert image to np array\n",
    "    # Preload images to memory (trying to speed things up)\n",
    "    all_files = glob.glob('{}*'.format(dataset_path))\n",
    "    # Gather ids and timestep info    \n",
    "    max_t = {}\n",
    "    for f in all_files:\n",
    "        regex = r\".*/{}/(.*-.*)-(.*).png\".format(dataset_id)\n",
    "        match = re.match(regex, f)\n",
    "        img_id, t = match.groups()\n",
    "        t = int(t)\n",
    "        max_t[img_id] = max_t[img_id] if img_id in max_t and max_t[img_id] > t else t   \n",
    "    \n",
    "    img_size = 28 * 28  # Pixel count\n",
    "    n_revisions = max(max_t.values()) + 1\n",
    "    n_items = len(max_t.values())\n",
    "    vs = np.empty((n_revisions, n_items, img_size))\n",
    "    \n",
    "    # Populate vs\n",
    "    for i, img_id in enumerate(natsorted(max_t)):\n",
    "        # Copy existing bitmaps to np.array\n",
    "        for t in range(0, max_t[img_id]):\n",
    "            img_file = dataset_path + img_id + '-' + str(t) + '.png'\n",
    "            vs[t][i] = (cv2.imread(img_file, cv2.IMREAD_GRAYSCALE) / 255.).flatten()\n",
    "        # Replicate last image\n",
    "        for t in range(max_t[img_id], n_revisions):\n",
    "            img_file = dataset_path + img_id + '-' + str(max_t[img_id]-1) + '.png'\n",
    "            vs[t][i] = (cv2.imread(img_file, cv2.IMREAD_GRAYSCALE) / 255.).flatten()    \n",
    "    return vs, list(natsorted(max_t)), n_revisions\n",
    "\n",
    "\n",
    "def tabular_dataset_to_array(dataset_path):\n",
    "    # Get files with coords and save in an array vs\n",
    "    all_files = natsorted(glob.glob('{}*'.format(dataset_path)))\n",
    "    vs = [pd.read_csv(f, index_col=0).values for f in all_files] \n",
    "    # Get dataset info \n",
    "    df_temp = pd.read_csv(all_files[0], index_col=0)\n",
    "    n_timesteps = len(all_files)\n",
    "    return np.array(vs), list(df_temp.index), n_timesteps\n",
    "\n",
    "\n",
    "def dataset_as_array(dataset_path):\n",
    "    if dataset_id in IMAGE_DATASETS:\n",
    "         return image_dataset_to_array(dataset_path)\n",
    "    else:\n",
    "        return tabular_dataset_to_array(dataset_path)\n",
    "\n",
    "\n",
    "def get_nd_mov(dataset_id):\n",
    "    mov = []\n",
    "    dataset_path = './Datasets/' + dataset_id + '/'\n",
    "    # Get the nd data into arrays\n",
    "    vs, indexes, n_timesteps = dataset_as_array(dataset_path)\n",
    "    # Compute dists between 2 nd arrays\n",
    "    for t in range(n_timesteps - 1):\n",
    "        v_t = vs[t]\n",
    "        v_tp1 = vs[t+1]\n",
    "        mov_t = []\n",
    "        for a, b in zip(v_t, v_tp1):\n",
    "            mov_t.append(math.sqrt(np.sum(np.square(a - b))))\n",
    "        mov.append(np.array(mov_t)) \n",
    "    return np.array(mov).T, indexes, n_timesteps\n",
    "\n",
    "# dists, indexes, n_timesteps = get_nd_dists('quickdraw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances\n",
    "mov_nd, indexes, n_timesteps = get_nd_mov(dataset_id)\n",
    "mov_md_dict = {}\n",
    "for p in projection_paths:\n",
    "    mov, _, _ = get_md_mov(p)\n",
    "    mov_md_dict[p] = mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_ids = ['stab_pearson', 'stab_spearman', 'stab_kendall', 'stab_kl', 'stab_stress_n', 'stab_stress_s',\n",
    "              'spat_pearson', 'spat_spearman', 'spat_kendall', 'spat_kl', 'spat_stress_n', 'spat_stress_s']\n",
    "\n",
    "for k in K_VALUES:\n",
    "    metric_ids.append('spat_np_' + str(int(k*10)))\n",
    "    metric_ids.append('spat_nh_' + str(int(k*10)))\n",
    "\n",
    "metric_results = pd.DataFrame(np.zeros((len(projection_paths), len(metric_ids))),\n",
    "                              index=projection_paths, columns=metric_ids)\n",
    "metric_results = metric_results.reindex(natsorted(metric_results.columns), axis=1)\n",
    "# metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "mov_nd = mov_nd.flatten()\n",
    "for p in projection_paths:\n",
    "    mov_md = mov_md_dict[p].flatten()\n",
    "\n",
    "    # Correlation and divergence metrics\n",
    "    metric_results.loc[p]['stab_pearson']  = scipy.stats.pearsonr(mov_nd, mov_md)[0]\n",
    "    metric_results.loc[p]['stab_spearman'] = scipy.stats.spearmanr(mov_nd, mov_md)[0]\n",
    "    metric_results.loc[p]['stab_kendall']  = scipy.stats.kendalltau(mov_nd, mov_md)[0]\n",
    "    metric_results.loc[p]['stab_kl']       = scipy.stats.entropy(mov_nd, mov_md)\n",
    "\n",
    "    # Stress metrics\n",
    "    nd = mov_nd / max(mov_nd)\n",
    "    md = mov_md / max(mov_md)\n",
    "    metric_results.loc[p]['stab_stress_n'] = np.sum(np.square(nd - md)) / np.sum(np.square(nd))\n",
    "\n",
    "    nd = (mov_nd - np.mean(mov_nd)) / np.std(mov_nd)\n",
    "    md = (mov_md - np.mean(mov_md)) / np.std(mov_md)\n",
    "    metric_results.loc[p]['stab_stress_s'] = np.sum(np.square(nd - md)) / np.sum(np.square(nd))\n",
    "\n",
    "# display(metric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(figsize=(len(projection_paths)*4,4), ncols=len(projection_paths))\n",
    "\n",
    "# mov_nd = mov_nd.flatten()\n",
    "# for i, p in enumerate(projection_paths):\n",
    "#     mov_md = mov_md_dict[p].flatten()\n",
    "#     axes[i].plot(mov_nd, mov_md, 'ko', alpha=.02)\n",
    "#     axes[i].set_title(p.split('/')[-1][:-4])\n",
    "#     if i == 0:\n",
    "#         axes[i].set_xlabel('nD movement')\n",
    "#         axes[i].set_ylabel('mD movement')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_path = './Datasets/' + dataset_id + '/'\n",
    "vs_n, indexes, n_revisions = dataset_as_array(dataset_path)\n",
    "\n",
    "for p in projection_paths:\n",
    "    # Change axis in projection\n",
    "    vs_m, _, _ = get_projection_as_array(p)\n",
    "    vs_m = np.transpose(vs_m, (1,0,2))\n",
    "    \n",
    "    # Initialize structures\n",
    "    index_to_class = np.array([item.split('-')[0] for item in np.array(indexes)])\n",
    "    ngbr_preservation = np.zeros((n_timesteps, len(indexes), len(K_VALUES)))\n",
    "    ngbr_hit = np.zeros((n_timesteps, len(indexes), len(K_VALUES)))\n",
    "    dists_nd_all = []\n",
    "    dists_md_all = [] \n",
    "    \n",
    "    for t in tqdm(range(n_timesteps)):\n",
    "        # Generate list of nearest neighbors for each item in timestep t\n",
    "        dists_nd, nbrs_nd = NearestNeighbors(n_neighbors=len(indexes), metric='euclidean',\n",
    "                                      algorithm='ball_tree').fit(vs_n[t]).kneighbors(vs_n[t])   \n",
    "        dists_md, nbrs_md = NearestNeighbors(n_neighbors=len(indexes), metric='euclidean',\n",
    "                                      algorithm='kd_tree').fit(vs_m[t]).kneighbors(vs_m[t])\n",
    "        \n",
    "        # Save distances to compute correlation/stress metrics\n",
    "#         dists_nd = np.delete(dists_nd, 0)\n",
    "        dists_nd_all.append(dists_nd)\n",
    "#         dists_md = np.delete(dists_md, 0)\n",
    "        dists_md_all.append(dists_md)\n",
    "        \n",
    "        # Check classes of neighbors for neighbor hit metric\n",
    "        for i in range(len(indexes)):\n",
    "            i_class = index_to_class[i]\n",
    "            k_max = int(max(K_VALUES) * len(indexes))\n",
    "            ngbr_classes = index_to_class[nbrs_md[i, :k_max]]\n",
    "            for k_index, k_percentage in enumerate(K_VALUES):\n",
    "                k = int(k_percentage * len(indexes))\n",
    "                ngbr_classes = ngbr_classes[:k]\n",
    "                nh = sum(map(lambda x: x == i_class, ngbr_classes)) / float(k)\n",
    "                ngbr_hit[t][i][k_index] = nh  \n",
    "\n",
    "        # Compute neighbor preservation for different values of k for each item \n",
    "        for i in range(len(indexes)):\n",
    "            for k_index, k_percentage in enumerate(K_VALUES):\n",
    "                k = int(k_percentage * len(indexes))\n",
    "                intersection = np.intersect1d(nbrs_nd[i, :k], nbrs_md[i, :k], assume_unique=True)\n",
    "                ngbr_preservation[t][i][k_index] = len(intersection) / float(k)\n",
    "        \n",
    "    # Average values over TIME (axis 0)\n",
    "    ngbr_preservation = np.average(ngbr_preservation, axis=0)\n",
    "    ngbr_hit = np.average(ngbr_hit, axis=0)\n",
    "\n",
    "    # Then average values over all points (new axis 0)\n",
    "    ngbr_preservation = np.average(ngbr_preservation, axis=0)\n",
    "    ngbr_hit = np.average(ngbr_hit, axis=0)\n",
    "\n",
    "    # We get one value per k\n",
    "    for i, k in enumerate(K_VALUES):\n",
    "        metric_results.loc[p]['spat_np_' + str(int(k*10))] = ngbr_preservation[i]\n",
    "        metric_results.loc[p]['spat_nh_' + str(int(k*10))] = ngbr_hit[i]\n",
    "    \n",
    "    dists_nd_all = np.array(dists_nd_all)\n",
    "    dists_md_all = np.array(dists_md_all)\n",
    "    dists_nd_all = dists_nd_all.flatten()\n",
    "    dists_md_all = dists_md_all.flatten()\n",
    "\n",
    "    # Stress metrics\n",
    "    nd = dists_nd_all / max(dists_nd_all)\n",
    "    md = dists_md_all / max(dists_md_all)\n",
    "    metric_results.loc[p]['spat_stress_n'] = np.sum(np.square(nd - md)) / np.sum(np.square(nd))\n",
    "\n",
    "    nd = (dists_nd_all - np.mean(dists_nd_all)) / np.std(dists_nd_all)\n",
    "    md = (dists_md_all - np.mean(dists_md_all)) / np.std(dists_md_all)\n",
    "    metric_results.loc[p]['spat_stress_s'] = np.sum(np.square(nd - md)) / np.sum(np.square(nd))\n",
    "    \n",
    "    # Correlation and divergence metrics\n",
    "    metric_results.loc[p]['spat_pearson']  = scipy.stats.pearsonr(dists_nd_all, dists_md_all)[0]\n",
    "    metric_results.loc[p]['spat_spearman'] = scipy.stats.spearmanr(dists_nd_all, dists_md_all)[0]\n",
    "    metric_results.loc[p]['spat_kendall']  = scipy.stats.kendalltau(dists_nd_all, dists_md_all)[0]\n",
    "    metric_results.loc[p]['spat_kl']       = scipy.stats.entropy(dists_nd_all, dists_md_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results.to_csv('./Metrics/Results/{}.csv'.format(dataset_id))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
